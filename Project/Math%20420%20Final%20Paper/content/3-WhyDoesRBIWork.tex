%----------------------------------------------------------------------------------------
% Why Does RBI Work
%----------------------------------------------------------------------------------------

\section{Why Does RBI Work?}

\subsection{Layman's Terms}

Let’s assume we're conducting a simple linear regression. Suppose that our model failed the the homoscedasticity and normality of residuals conditions, our data is random, and we have n > 30 observations in our dataset. By the central limit theorem, we can then assume our slope coefficient, $\beta_1$ is approximately normally distributed. However, we can’t wholly trust the p-value associated with that slope. This is because for statistical inference, we need certain conditions to be met depending on the test. In this example of linear regression, we need a independence, a linear relationship between our response and predictor, and homoscedasticity of residuals. If it’s the case where these conditions cannot be met but we do have a sample representative of the population, we can create our randomization model. This randomization model is the distribution of n test statistics that we get from scrambling the data n times and calculating the given test statistic. Our randomization model also assumes the null hypothesis of the test statistic being zero or no statistically significant effect being present, creating what is called a null distribution. This is because the model has no parameters and that each time the data is shuffled, the observations become independent of their outcomes. Our original model, and typically most models, are tested at a significance level of $\alpha=.05$. Because our randomization model assumes observations are independent of their outcomes, a constant significance level, or type 1 error rate, is held constant for the randomization model. Furthermore, by creating this randomization model, our randomization distribution is approximately the null distribution of whatever statistic we're calculating. Now, we calculate the p-value from the proportion of test statistics calculated from our randomly permuted dataset that are as or more extreme than our original test statistic. We can trust this p-value because the p-value comes from a now uniform distribution. 



\subsection{Sharp Null Hypothesis \& Asymptotic Normality and p-value}

As with many concepts in statistics and probability, the basis for randomization-based inference lies in the Central Limit Theorem. Let's stick with our simple linear regression example from above. As we saw, when we permute our data in simple linear regression, we are basically saying that there is independence between $x$ and $y$. This is equivalent to the null hypothesis, $H_0:\hat{\beta_1}=0$. In fact, for many permutations, this is equivalent to Fischer's sharp null hypothesis which says that there is no effect for each pairwise set of $x$ and $y$ values. Traditionally Fischer's exact test is defined as 
$$H_0:Y_i(1)=Y_i(0)\quad\forall i=1,2,\dots,N \text{ for } N \text{ many permutations.}$$ As our example is dealing with linear regression we can't have this specific null hypothesis, but instead we can interpret it as $$H_0:\hat{\beta_1}_{,i}=0\quad\forall i = 1,2,\dots,N.$$ Now, let's recall the Central Limit Theorem. The CLT says that given a population with mean $\mu$ and standard deviation $\sigma$ and we take sufficiently large random samples from the population with replacement, then our sampling distribution will be approximately normally distributed about $\mu$. What does this mean for our case of simple linear regression? Well, we can think about the population of least squares estimators as every $\hat{\beta_1}$ we would get if were to permute the response column. A critical part of this is that our data from which we are calculating the least squares estimator is a representative random sample from the population. If so, then we can validate this claim that our distribution of least squares estimators is representative of the population as required by the CLT. This is crucial if we are to conduct statistical inference as randomization-based inference spells out. This is the reason we are able to generalize and make inferences about the population, even when model conditions aren't met. Now, with this in mind, each time we take a permutation of our data, we are taking a random sample from the population of $\hat{\beta_1}\text{'s}$. As we saw above, the permutations we make lead to the assumption of the null hypothesis, $H_0:\hat{\beta_1}=0$. Therefore, by the central limit theorem, our reference distribution will be centered around zero because under the null hypothesis we have $E(\beta_1)=0$ and for a normal distribution we have $E(X)=\mu$. If we take all $n!$ permutations of an $n$-length response vector we would have an approximately normal distribution. As we take more permutations and include each newly calculated test statistic in the randomization distribution, we will only further approach normality to the point where more permutations with neither help nor hurt the normality of our reference distribution. This type of distribution is called an asymptotic distribution, thus, we have asymptotic normality in our reference distribution.
\newline\\
Since our distribution in asymptotically normal, our p-value is not only valid, but it will be approximately exact, depending on the number of permutations that we run. If we reach the point where we have approached asymptotic normality, we would not just be approximating a p-value through statistical theory, but actually calculating it on an asymptotic normal distribution. By the same virtue as we showed asymptotic normality, we have an asymptotic p-value since we are calculating a p-value from this asymptotic distribution. We cannot take any more permutations that will influence the p-value since the distribution we are calculating the p-value on is nearly fixed for large numbers of permutations. 

\subsection{In General}

This process that we took is true for many test statistics that we can choose. If we know that our sample is truly an approximation of the population, we are able to construct a roughly normal randomization centered around the null hypothesis of our test. Examples of other test statistics that we can use this approach for are means, proportions, difference in means, difference in proportions, and correlations. The same is true for $\chi^2$- and $F$-distributions, however there is a bit more legwork that needs to happen before we can see this clearly. The formula for a $\chi^2$ test statistic is $$\sum\frac{(O_i-E_i)}{E_i}$$ where $O_i$ is each observed data point and $E_i$ is the expected value for that group. Assuming that our data has been randomly sampled from the population, one of the assumptions that we hold for randomization based inference, then $O_i$ will be normal random variable meaning the difference $O_i-E_i$ is also a normal random variable. Assuming the null hypothesis  of no relationship (as we do when we shuffle the data), $E(O_i-E_i)=0$. Thus, the $\chi^2$-distribution will be a sum of squared normal random variables taken from a standard normal distribution. As we shuffle the data and  sample more normal random variables from the standard normal distribution, the sum of their squares will only further approximate a $\chi^2$-distribution. From this randomization distribution we can calculate a p-value by taking the proportion of $\chi^2$ statistics that are greater than what we observed in our original sample over the total number of permutations. 
\newline\\
Similarly, we can show that our randomized $F$-distribution will in fact be $F$-distributed using the same reasoning as we did previously to show that random permutations of our data will lead to a $\chi^2$-distribution. Again, we're assuming the null in this case when we randomize our data. For an $F$-test, the null hypothesis is that both models or groups are the same. When we calculate our proportion that we do for an $F$-statistic, we will be taking a proportion of $\chi^2$ random variables (multiplied by a factor of the inverse of each $\chi^2$ random variable's degrees of freedom). By definition, this will be an $F$ random variable. Thus with many permutation we will have the null distribution of $F$-statistics which is centered at 1. Since we've satisfied that we can approximate our necessary distribution, we can calculate a p-value just as above. 