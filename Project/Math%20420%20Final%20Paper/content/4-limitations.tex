%----------------------------------------------------------------------------------------
% Limitations
%----------------------------------------------------------------------------------------

\section{Limitations}

Although what we have discussed in this paper so far is robust, there are some chinks in Randomization Based Inference’s armor. 

\begin{enumerate}
    \item \textbf{Random Data Condition} \newline
	The first and most glaring limitation of RBI is that it is not truly conditionless. The one condition that it does have is that the initial data has to be random. Some basic cases where this is not met is when you have access to the whole population, in which case you do not need randomization based inference, or when collecting data you failed to use a randomization procedure. The latter is the situation we are more focused on in this paper. In general, when a sample fails to be random, it is no longer assumed to be representative of the population. 
	We can connect this to randomization based inference in that we are trying to find an asymptotically correct p-value. In order to do this, we have to randomly shuffle the data and extract our test statistic and build the randomization distribution and then calculate the p-value from that. This process becomes invalid when the original data fails to be random because of two key issues. We can start with the simpler one: The original sample contains high levels of bias. Let’s assume that we are trying to figure out if a bill on an upcoming ballot does not have equal support in a community. Instead of using your city hall and a random number generator to sample people, you decide to ask everyone who walks into the local Hobby Lobby. In your mind, this seems reasonable, considering that you don’t know who is going to walk in and what they are thinking about the bill. However, you have failed to consider the population of people who shop at Hobby Lobby. This can introduce bias into your sample. It could be response bias or under-coverage bias (specific groups of people left out). The fatal flaw that both the former and the latter present is that they alter the null hypothesis of your test. If you were to continue with your experiment and use randomization based inference, you won’t be able to trust any of your permuted test statistics and your final p-value due to the bias present. \newline\\
	The second and more complex issue of not obtaining a random sample focuses on the obvious connection between a random sample and randomization based inference. The main point to be made here is that if a sample is not random, we cannot assume it to be representative of the population our sample comes from. In the Hobby Lobby story, the sample taken could be representative of the population. Here, even if some randomized sampling procedure is taken, the sample could still be not random. Typically when we code in R, we use tests like Bartel’s test for randomness to determine if data has been sampled randomly. If the test fails to reject the null hypothesis that the data is random, then we are fair to move forward with randomization-based inference. When the null hypothesis is rejected, randomization-based inference will not work. This is due to the fact that we are typically using a test to make an inference about the population. We use the term correlation, because our tests can’t conclude causation. Regardless of the prior statement, the main problem here is that our random variables, no matter the type of test, are not approximately normally distributed. This is because the Central Limit Theorem needs randomly distributed variables for it to apply. When a sample is not random, the variables we are observing are also not random. This makes the random variable we're observing to be not approximately normally distributed. No matter the test statistic calculated we will not be allowed us to trust our test statistic, causing a breakdown of the entire randomization process. \newline\\
	In summation, in order for our randomization based inference to be of value, we need a sample free of bias and that is random. This ensures that our variables are random variables and we can trust the test statistic created from them. 
    \item \textbf{Small Sample Sizes} \newline
	For most tests, there is typically a condition that requires the initial sample to be of an acceptable size. For example, when trying to determine if two different treatment groups have a difference in means that is not equal to zero, we require that for the Central Limit Theorem to work, we need at least 30 data points, assuming our sample is random. Although randomization based inference can generate many permutations of a given sample, the one thing it can’t do is increase the sample size. The randomization based inference does not deal with the counterfactual statement that “if the sample size was larger, we could be more sure of our conclusion”. Instead, the point here is that our randomization distribution using a small original sample size will be less approximately normal than a satisfactory sample size. This affects the robustness of our randomization based inference. Sure, it created something that could pass as normal to any given statistician, but the level of asymptotic certainty we have is in jeopardy. 
	\item \textbf{Population Parameter Estimation} \newline
	One of the important aspects of model creation in statistics is the idea of parameter estimation and statistical accuracy. We often fit one model at the beginning of our experiment and then go through rigorous testing and fitting procedures to develop the best model. This best model is not only accurate at its forecasting and initial predictions, but also does not over-fit to the data so that it can be generalized to new data. The limitation we are getting at here is that randomization based inference is not something we can use to develop better population parameter estimates. \newline\\
	In general, we use randomization based inference to help us figure out if the perceived effects we see in our original model are trustworthy. However, if we are doing linear regression, our beta estimates are only as good as the original sample we have. In randomization based inference, we resample data without replacement. This means we assume our null hypothesis is true. However, if we wanted to make sure our sample statistics were better at estimating the true population parameters given our test, we would have to take a different approach. Instead of just randomly shuffling our response variable, we could take all of our observations and pool them back up, then choose one response value at random and assign it to a given observation. We then could put the response value back into the pool of responses and repeat the process. This is sampling with replacement. Instead of building a randomization distribution which looks to estimate the distribution of the given test statistic, this estimates the sampling distribution. This estimated sampling distribution is centered around our original sampling mean and has a standard deviation from our original sample as well. However, with this distribution, we can make confidence intervals that give us more precise estimates of the population parameter. This process is referred to as bootstrapping. In most cases, we should really pair these models so that we can have better model accuracy as well as more clarity around the effect we are testing for. However, the computational acumen required for both tests is quite high. Lastly, the level of interpretability when using both methods of resampling is murky due to the different procedures used. Depending on what your goal is, picking one of these methods will be more useful than including both. 
\end{enumerate}
